{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Cleaning and EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StartupSuccessPrediction\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'investments_VC.csv'\n",
    "data = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "# Strip whitespace from column names\n",
    "data.columns = [col.strip() for col in data.columns]\n",
    "\n",
    "# Replace commas and '-' characters, and convert to numeric\n",
    "data['funding_total_usd'] = pd.to_numeric(data['funding_total_usd'].str.replace(',', '').replace('-', ''), errors='coerce')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "threshold = len(data) * 0.5\n",
    "data = data.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Fill missing values with appropriate strategies (mean, median, mode, etc.)\n",
    "data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Display basic statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Save for Tableau visualization\n",
    "data.to_csv('data/cleaned_investments_VC.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Investment: 650933703144.0\n",
      "Average Investment: 15912526.05040702\n",
      "Median Investment: 2000000.0\n",
      "Number of Sectors: 753\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'investments_VC.csv'\n",
    "data = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "# Strip whitespace from column names\n",
    "data.columns = [col.strip() for col in data.columns]\n",
    "\n",
    "# Replace commas and '-' characters, and convert to numeric\n",
    "data['funding_total_usd'] = pd.to_numeric(data['funding_total_usd'].str.replace(',', '').replace('-', ''), errors='coerce')\n",
    "\n",
    "# Calculate total investment\n",
    "total_investment = data['funding_total_usd'].sum()\n",
    "print(f\"Total Investment: {total_investment}\")\n",
    "\n",
    "# Calculate average investment\n",
    "average_investment = data['funding_total_usd'].mean()\n",
    "print(f\"Average Investment: {average_investment}\")\n",
    "\n",
    "# Calculate median investment\n",
    "median_investment = data['funding_total_usd'].median()\n",
    "print(f\"Median Investment: {median_investment}\")\n",
    "\n",
    "# Calculate number of unique sectors\n",
    "num_sectors = data['market'].nunique()\n",
    "print(f\"Number of Sectors: {num_sectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering and Data Transformation using PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- homepage_url: string (nullable = true)\n",
      " |-- category_list: string (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- funding_total_usd: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- funding_rounds: double (nullable = true)\n",
      " |-- founded_at: date (nullable = true)\n",
      " |-- founded_month: timestamp (nullable = true)\n",
      " |-- founded_quarter: string (nullable = true)\n",
      " |-- founded_year: double (nullable = true)\n",
      " |-- first_funding_at: date (nullable = true)\n",
      " |-- last_funding_at: date (nullable = true)\n",
      " |-- seed: double (nullable = true)\n",
      " |-- venture: double (nullable = true)\n",
      " |-- equity_crowdfunding: double (nullable = true)\n",
      " |-- undisclosed: double (nullable = true)\n",
      " |-- convertible_note: double (nullable = true)\n",
      " |-- debt_financing: double (nullable = true)\n",
      " |-- angel: double (nullable = true)\n",
      " |-- grant: double (nullable = true)\n",
      " |-- private_equity: double (nullable = true)\n",
      " |-- post_ipo_equity: double (nullable = true)\n",
      " |-- post_ipo_debt: double (nullable = true)\n",
      " |-- secondary_market: double (nullable = true)\n",
      " |-- product_crowdfunding: double (nullable = true)\n",
      " |-- round_A: double (nullable = true)\n",
      " |-- round_B: double (nullable = true)\n",
      " |-- round_C: double (nullable = true)\n",
      " |-- round_D: double (nullable = true)\n",
      " |-- round_E: double (nullable = true)\n",
      " |-- round_F: double (nullable = true)\n",
      " |-- round_G: double (nullable = true)\n",
      " |-- round_H: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "df = spark.read.csv('data/cleaned_investments_VC.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df = df.select([col(c).alias(c.strip()) for c in df.columns])\n",
    "\n",
    "# Display the schema of the dataset\n",
    "df.printSchema()\n",
    "\n",
    "# Handling missing values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Feature Engineering\n",
    "# Example: Creating a new column 'is_successful' based on funding rounds\n",
    "df = df.withColumn(\"is_successful\", when(col(\"status\") == \"operating\", 1).otherwise(0))\n",
    "\n",
    "# Selecting relevant features for prediction\n",
    "selected_columns = ['name', 'funding_total_usd', 'founded_year', 'first_funding_at', 'last_funding_at', 'is_successful']\n",
    "df = df.select(selected_columns)\n",
    "\n",
    "# Convert to Pandas DataFrame and save as CSV\n",
    "df_pd = df.toPandas()\n",
    "df_pd.to_csv('data/engineered_investments_VC.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning to Predict Startup Success using PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.5852110216357815\n",
      "+-------------------+-----------------+------------+-----------------+--------------------+--------------+\n",
      "|               name|funding_total_usd|founded_year|predicted_success|         probability|actual_success|\n",
      "+-------------------+-----------------+------------+-----------------+--------------------+--------------+\n",
      "|       1000 Markets|         500000.0|        2009|              1.0|[0.48845765784664...|             0|\n",
      "|          12Society|         619494.0|        2012|              1.0|[0.47175117237640...|             0|\n",
      "|         1C Company|            2.0E8|        1991|              0.0|[0.57005518902110...|             0|\n",
      "|              1Cast|          40000.0|        2006|              0.0|[0.50522116295291...|             0|\n",
      "|        24PageBooks|          50000.0|        2010|              1.0|[0.48292893993981...|             0|\n",
      "|            25eight|          25000.0|        2012|              1.0|[0.47180546621406...|             0|\n",
      "|           27 Perry|         118000.0|        2012|              1.0|[0.47179697268328...|             0|\n",
      "|     2Win-Solutions|          80000.0|        2012|              1.0|[0.47180044315625...|             0|\n",
      "|       3 day Blinds|          80000.0|        1984|              0.0|[0.62514157170807...|             0|\n",
      "|3D FUTURE VISION II|         503000.0|        2011|              1.0|[0.47732183518662...|             0|\n",
      "|                3LM|        1500000.0|        2010|              1.0|[0.48279624743920...|             0|\n",
      "|         3i Systems|       1.370815E7|        2006|              0.0|[0.50396899755248...|             0|\n",
      "|              3nder|        1500000.0|        2014|              1.0|[0.46057334168121...|             0|\n",
      "|     41st Parameter|       3.806457E7|        2004|              0.0|[0.51288376759860...|             0|\n",
      "|              4Blox|            1.0E8|        2006|              1.0|[0.49606312875492...|             0|\n",
      "|         500Friends|           1.29E7|        2010|              1.0|[0.48175309585462...|             0|\n",
      "|            51 Auto|        4500000.0|        2009|              1.0|[0.48809138060185...|             0|\n",
      "|         5min Media|           1.28E7|        2007|              1.0|[0.49847775645431...|             0|\n",
      "|6th Sense Analytics|        6700000.0|        2004|              0.0|[0.51575501533265...|             0|\n",
      "|   7 Billion People|        6528902.0|        2006|              0.0|[0.50462670941578...|             0|\n",
      "+-------------------+-----------------+------------+-----------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ensure necessary columns are cast to the correct types\n",
    "df = df.withColumn(\"funding_total_usd\", col(\"funding_total_usd\").cast(\"double\"))\n",
    "df = df.withColumn(\"founded_year\", col(\"founded_year\").cast(\"integer\"))\n",
    "\n",
    "# Fill null values\n",
    "df = df.fillna({'funding_total_usd': 0, 'founded_year': 0})\n",
    "\n",
    "# Selecting relevant features for prediction\n",
    "selected_columns = ['name', 'funding_total_usd', 'founded_year', 'is_successful']\n",
    "df = df.select(selected_columns)\n",
    "\n",
    "# Count the number of 1s and 0s in the 'is_successful' column\n",
    "num_zeros = df.filter(col('is_successful') == 0).count()\n",
    "num_ones = df.filter(col('is_successful') == 1).count()\n",
    "\n",
    "# Calculate the ratio of 1s to 0s\n",
    "ratio = num_zeros / num_ones\n",
    "\n",
    "# Oversample the minority class (1s)\n",
    "df_ones = df.filter(col('is_successful') == 1)\n",
    "df_zeros = df.filter(col('is_successful') == 0)\n",
    "\n",
    "df_ones_oversampled = df_ones.sample(withReplacement=True, fraction=ratio, seed=42)\n",
    "\n",
    "# Combine the oversampled 1s and the original 0s\n",
    "df = df_zeros.union(df_ones_oversampled)\n",
    "\n",
    "# VectorAssembler to combine feature columns into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=['funding_total_usd', 'founded_year'], outputCol='features')\n",
    "\n",
    "# Check if 'features' column already exists and drop it if necessary\n",
    "if 'features' in df.columns:\n",
    "    df = df.drop('features')\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(labelCol='is_successful', featuresCol='features')\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='is_successful')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Rename 'is_successful' to 'actual_success' and 'prediction' to 'predicted_success'\n",
    "predictions = predictions.withColumnRenamed('is_successful', 'actual_success')\n",
    "predictions = predictions.withColumnRenamed('prediction', 'predicted_success')\n",
    "\n",
    "# Print the results\n",
    "predictions.select(\"name\", \"funding_total_usd\", \"founded_year\", \"predicted_success\", \"probability\", \"actual_success\").show()\n",
    "\n",
    "# Convert predictions to Pandas DataFrame and save for Tableau visualization\n",
    "predictions_pd = predictions.select(\"name\", \"funding_total_usd\", \"founded_year\", \"predicted_success\", \"probability\", \"actual_success\").toPandas()\n",
    "predictions_pd.to_csv('data/predictions_investments_VC.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
